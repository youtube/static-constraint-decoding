{"cells":[{"cell_type":"markdown","metadata":{"id":"eBMoZCIlrOZl"},"source":["# STATIC Constrained Decoding with OpenOneRec-1.7B\n","\n","This notebook demonstrates **STATIC** (Sparse Transition-Accelerated Trie Index for Constrained Decoding) applied to a real generative retrieval model — [OneRec-1.7B](https://huggingface.co/OpenOneRec/OneRec-1.7B) from OpenOneRec.\n","\n","**Key idea:** OneRec generates 3-level Semantic IDs (SIDs) to retrieve items. STATIC enforces that every decoded SID belongs to a valid item catalog, using a hybrid dense + CSR constraint index that runs entirely on GPU with zero Python-loop overhead.\n","\n","We use **synthetic SIDs** (real item catalogs aren't publicly available) to demonstrate that constraint enforcement works correctly with real model logits."]},{"cell_type":"markdown","metadata":{"id":"PIhTXGmkrOZo"},"source":["## 1. Install & Imports"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"fI1S7AjArOZp","executionInfo":{"status":"ok","timestamp":1772138579278,"user_tz":300,"elapsed":11857,"user":{"displayName":"Yueqi Wang","userId":"16098646395460348304"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"b17178d8-b782-4e62-bc34-a8506141842c"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[31mERROR: static-constraint-decoding is not a valid editable requirement. It should either be a path to a local project or a VCS URL (beginning with bzr+http, bzr+https, bzr+ssh, bzr+sftp, bzr+ftp, bzr+lp, bzr+file, git+http, git+https, git+ssh, git+git, git+file, hg+file, hg+http, hg+https, hg+ssh, hg+static-http, svn+ssh, svn+http, svn+https, svn+svn, svn+file).\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["!pip install -q transformers accelerate\n","!git clone https://github.com/google-research/static-constraint-decoding.git 2>/dev/null; pip install -q -e static-constraint-decoding"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"9MU_zPg4rOZq","executionInfo":{"status":"error","timestamp":1772138650158,"user_tz":300,"elapsed":38,"user":{"displayName":"Yueqi Wang","userId":"16098646395460348304"}},"outputId":"1aa91683-5910-457c-dead-fd5ce59f4394","colab":{"base_uri":"https://localhost:8080/","height":385}},"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'static_decoding'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-907/4165565417.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mstatic_decoding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsr_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbuild_static_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstatic_decoding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoding_pt\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msparse_transition_torch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'static_decoding'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["import torch\n","import torch.nn as nn\n","import numpy as np\n","from static_decoding.csr_utils import build_static_index\n","from static_decoding.decoding_pt import sparse_transition_torch\n","from transformers import AutoModelForCausalLM, AutoTokenizer"]},{"cell_type":"markdown","metadata":{"id":"ZTCiIpw7rOZq"},"source":["## 2. Load OneRec-1.7B\n","\n","OneRec-1.7B is a Qwen3-based model fine-tuned for generative retrieval. Its vocabulary extends Qwen3's base vocab (151,669 tokens) with three levels of SID tokens:\n","\n","| Level | Tokens | Model IDs |\n","|-------|--------|-----------|\n","| a | `<s_a_0>` ... `<s_a_8191>` | `[base, base+8192)` |\n","| b | `<s_b_0>` ... `<s_b_8191>` | `[base+8192, base+16384)` |\n","| c | `<s_c_0>` ... `<s_c_8191>` | `[base+16384, base+24576)` |\n","\n","followed by `<|sid_begin|>` and `<|sid_end|>` delimiters."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oxQa2m6DrOZq"},"outputs":[],"source":["MODEL_NAME = \"OpenOneRec/OneRec-1.7B\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n","model = AutoModelForCausalLM.from_pretrained(\n","    MODEL_NAME, torch_dtype=torch.float16, device_map=\"auto\", trust_remote_code=True\n",")\n","model.eval()\n","\n","device = next(model.parameters()).device\n","print(f\"Model loaded on {device}\")\n","\n","# Dynamically detect SID token offsets from the tokenizer\n","s_a_base = tokenizer.convert_tokens_to_ids(\"<s_a_0>\")\n","s_b_base = tokenizer.convert_tokens_to_ids(\"<s_b_0>\")\n","s_c_base = tokenizer.convert_tokens_to_ids(\"<s_c_0>\")\n","sid_begin_id = tokenizer.convert_tokens_to_ids(\"<|sid_begin|>\")\n","sid_end_id = tokenizer.convert_tokens_to_ids(\"<|sid_end|>\")\n","\n","CODEBOOK_SIZE = 8192\n","assert s_b_base == s_a_base + CODEBOOK_SIZE, f\"SID levels not contiguous: a={s_a_base}, b={s_b_base}\"\n","assert s_c_base == s_b_base + CODEBOOK_SIZE, f\"SID levels not contiguous: b={s_b_base}, c={s_c_base}\"\n","\n","print(f\"SID offsets — Level a: {s_a_base}, Level b: {s_b_base}, Level c: {s_c_base}\")\n","print(f\"Special tokens — <|sid_begin|>: {sid_begin_id}, <|sid_end|>: {sid_end_id}\")"]},{"cell_type":"markdown","metadata":{"id":"86X_02VorOZr"},"source":["## 3. Generate Synthetic SIDs & Build STATIC Index\n","\n","Since the real item catalog is not publicly available, we generate ~50K random 3-level SIDs with `codebook_size=8192` and build the STATIC index with `d=2` dense layers (optimal for L=3: two dense layers cover levels 0–1, CSR handles level 2)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nvQrullBrOZr"},"outputs":[],"source":["N_ITEMS = 50_000\n","L = 3  # 3-level SID structure\n","\n","np.random.seed(42)\n","sids = np.random.randint(0, CODEBOOK_SIZE, size=(N_ITEMS, L), dtype=np.int32)\n","sids = np.unique(sids, axis=0)\n","sids = sids[np.lexsort([sids[:, i] for i in range(L - 1, -1, -1)])]\n","print(f\"Generated {len(sids)} unique SIDs of length {L}\")\n","\n","# Build the STATIC index (hybrid dense + CSR)\n","packed_csr, indptr, lmb, start_mask, dense_mask, dense_states = build_static_index(\n","    sids, vocab_size=CODEBOOK_SIZE, d=2\n",")\n","\n","# Move index tensors to GPU\n","packed_csr_t = torch.tensor(packed_csr, dtype=torch.long, device=device)\n","indptr_t = torch.tensor(indptr, dtype=torch.long, device=device)\n","start_mask_t = torch.tensor(start_mask, dtype=torch.bool, device=device)\n","dense_mask_t = torch.tensor(dense_mask, dtype=torch.bool, device=device)\n","dense_states_t = torch.tensor(dense_states, dtype=torch.long, device=device)\n","\n","print(f\"STATIC index built. Max branch factors per level: {lmb}\")"]},{"cell_type":"markdown","metadata":{"id":"OWGGItz-rOZr"},"source":["## 4. Define the OneRec–STATIC Wrapper\n","\n","STATIC's `sparse_transition_torch` expects a model that maps `(B, 1)` input tokens in SID-space `[0, 8192)` to `(B, 1, 8192)` logits. This wrapper bridges the gap:\n","\n","1. **Prefill** — On the first call, run the full prompt (including `<|sid_begin|>`) through the model and cache KV states. Return level-a logits.\n","2. **Autoregressive** — On subsequent calls, map SID-space tokens to model vocabulary IDs, feed through the model with KV cache, and return next-level logits.\n","3. **KV cache expansion** — When batch size jumps from B to B×beam_size, `repeat_interleave` the cache.\n","\n","> **Note on KV cache approximation:** Beam reordering between levels does not update the KV cache. For L=3, this affects only 1 step. Constraint enforcement is still 100% correct; only the model's logit *quality* for the final level is slightly degraded."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I-DxfeIcrOZr"},"outputs":[],"source":["class OneRecSTATICWrapper(nn.Module):\n","    \"\"\"Bridges STATIC's model interface with a HuggingFace causal LM.\n","\n","    STATIC operates in local SID-space [0, codebook_size). This wrapper:\n","    1. Prefills the prompt (incl. <|sid_begin|>) on the first call\n","    2. Maps SID-space tokens to model vocab IDs on subsequent calls\n","    3. Slices full-vocabulary logits to return only the relevant SID level\n","    4. Manages KV cache expansion when batch size grows for beam search\n","    \"\"\"\n","\n","    def __init__(self, hf_model, prompt_ids, sid_offsets, codebook_size=8192):\n","        super().__init__()\n","        self.hf_model = hf_model\n","        self.prompt_ids = prompt_ids      # (1, seq_len) including <|sid_begin|>\n","        self.sid_offsets = sid_offsets     # [s_a_base, s_b_base, s_c_base]\n","        self.codebook_size = codebook_size\n","        self.past_key_values = None\n","        self.call_idx = 0\n","\n","    def _slice_sid_logits(self, full_logits, level):\n","        \"\"\"Extract logits for the given SID level from full vocabulary logits.\"\"\"\n","        off = self.sid_offsets[level]\n","        return full_logits[:, :, off:off + self.codebook_size]\n","\n","    def _expand_kv_cache(self, target_B):\n","        \"\"\"Expand KV cache batch dimension via repeat_interleave.\"\"\"\n","        if self.past_key_values is None:\n","            return\n","        # Handle transformers DynamicCache (list-based)\n","        if hasattr(self.past_key_values, 'key_cache'):\n","            cur_B = self.past_key_values.key_cache[0].shape[0]\n","            if cur_B == target_B:\n","                return\n","            factor = target_B // cur_B\n","            for i in range(len(self.past_key_values.key_cache)):\n","                self.past_key_values.key_cache[i] = (\n","                    self.past_key_values.key_cache[i].repeat_interleave(factor, dim=0)\n","                )\n","                self.past_key_values.value_cache[i] = (\n","                    self.past_key_values.value_cache[i].repeat_interleave(factor, dim=0)\n","                )\n","        else:\n","            # Legacy tuple-of-tuples KV cache\n","            cur_B = self.past_key_values[0][0].shape[0]\n","            if cur_B == target_B:\n","                return\n","            factor = target_B // cur_B\n","            self.past_key_values = tuple(\n","                tuple(t.repeat_interleave(factor, dim=0) for t in layer)\n","                for layer in self.past_key_values\n","            )\n","\n","    def forward(self, input_ids):\n","        \"\"\"\n","        Args:\n","            input_ids: (B, 1) in SID-space [0, codebook_size).\n","        Returns:\n","            (B, 1, codebook_size) logits for the next SID level.\n","        \"\"\"\n","        B = input_ids.shape[0]\n","\n","        if self.call_idx == 0:\n","            # --- Prefill: run the full prompt through the model ---\n","            out = self.hf_model(self.prompt_ids, use_cache=True)\n","            self.past_key_values = out.past_key_values\n","            logits = out.logits[:, -1:, :]  # (1, 1, V_full)\n","            self.call_idx += 1\n","            return self._slice_sid_logits(logits, level=0)\n","\n","        # --- Autoregressive steps ---\n","        self._expand_kv_cache(B)\n","\n","        # Map SID-space token -> model vocabulary ID\n","        level_in = self.call_idx - 1\n","        model_ids = input_ids + self.sid_offsets[level_in]\n","\n","        out = self.hf_model(\n","            model_ids, past_key_values=self.past_key_values, use_cache=True\n","        )\n","        self.past_key_values = out.past_key_values\n","\n","        level_out = self.call_idx\n","        self.call_idx += 1\n","        return self._slice_sid_logits(out.logits, level=level_out)"]},{"cell_type":"markdown","metadata":{"id":"kwaQD2wDrOZs"},"source":["## 5. Run Constrained Decoding\n","\n","We skip the chain-of-thought reasoning stage and jump straight to SID generation. The prompt includes a fixed `<think>...</think>` block followed by `<|sid_begin|>` to trigger SID decoding."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wbxpHpOLrOZs"},"outputs":[],"source":["# Build prompt: skip CoT, go straight to SID decoding\n","prompt_text = (\n","    \"User: Recommend me a sci-fi movie.\\n\"\n","    \"Assistant: <think>The user wants a sci-fi movie. \"\n","    \"I will retrieve relevant items from the catalog.</think>\\n\"\n",")\n","prompt_ids = tokenizer.encode(prompt_text, return_tensors=\"pt\").to(device)\n","sid_begin_tensor = torch.tensor([[sid_begin_id]], device=device)\n","prompt_ids = torch.cat([prompt_ids, sid_begin_tensor], dim=1)\n","print(f\"Prompt length: {prompt_ids.shape[1]} tokens\")\n","\n","# Instantiate the wrapper\n","wrapper = OneRecSTATICWrapper(\n","    hf_model=model,\n","    prompt_ids=prompt_ids,\n","    sid_offsets=[s_a_base, s_b_base, s_c_base],\n","    codebook_size=CODEBOOK_SIZE,\n",")\n","\n","BEAM_SIZE = 10\n","\n","# Run STATIC constrained beam search\n","print(f\"Running constrained beam search (beam_size={BEAM_SIZE}, L={L})...\")\n","outputs = sparse_transition_torch(\n","    model=wrapper,\n","    batch_size=1,\n","    beam_size=BEAM_SIZE,\n","    tokens_per_beam=20,\n","    start_token=0,\n","    max_sample_len=L,\n","    vocab_size=CODEBOOK_SIZE,\n","    max_branch_factors=lmb,\n","    packed_csr=packed_csr_t,\n","    csr_indptr=indptr_t,\n","    start_mask=start_mask_t,\n","    dense_mask=dense_mask_t,\n","    dense_states=dense_states_t,\n","    device=device,\n","    d_dense=2,\n",")\n","\n","print(f\"\\nDecoded {outputs.shape[1]} beams of length {outputs.shape[2]}:\")\n","print(outputs[0].cpu().numpy())"]},{"cell_type":"markdown","metadata":{"id":"TGRHZXLOrOZs"},"source":["## 6. Verification\n","\n","Every decoded beam must be a member of the synthetic SID catalog — 100% constraint satisfaction."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-zg88Lj6rOZs"},"outputs":[],"source":["decoded_np = outputs[0].cpu().numpy()  # (beam_size, L)\n","valid_set = {tuple(row) for row in sids}\n","\n","valid_count = sum(1 for sid in decoded_np if tuple(sid) in valid_set)\n","print(f\"Verification: {valid_count}/{len(decoded_np)} decoded SIDs are in the valid set.\")\n","assert valid_count == len(decoded_np), \"Constraint violation detected!\"\n","print(\"All constraints satisfied.\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.0"}},"nbformat":4,"nbformat_minor":0}